{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Exploration\n",
    "\n",
    "Overview of today's topics:\n",
    "  - Data cleaning and feature engineering with real world data sets\n",
    "  - Exploring data sets with descriptive stats and visualization\n",
    "  \n",
    "To set this lecture up, I downloaded the most popular data sets from 1) LA's covid dashboard, 2) the LA city data portal, and 3) the LA county data portal. This gives us a variety of real-world data sets that are relatively messy and require some cleaning and transformation prior to analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data cleaning\n",
    "\n",
    "### 1.1. LA County Covid Cases\n",
    "\n",
    "[Data source](http://dashboard.publichealth.lacounty.gov/covid19_surveillance_dashboard/)\n",
    "\n",
    "Note from the Covid data source: \"Crude and Adjusted Rates are Per 100,000 population (2018 Population Estimates). Adjusted Rate is age-adjusted by year 2000 US Standard Population. Adjusted rates account for differences in the distribution of age in the underlying population. Adjusted rates are useful for comparing rates across geographies (i.e. comparing the rate between cities that have different age distributions).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = pd.read_csv('../../data/LA_County_Covid19_CSA_case_death_table.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what do you see in the raw data?\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data types: do we need to change/convert any?\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the duplicate IDs and rename the place column to something meaningful\n",
    "df = df.drop(columns=['Unnamed: 0']).rename(columns={'geo_merge':'place_name'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up place names\n",
    "df['place_name'] = df['place_name'].str.replace('City of ', '').str.replace('Unincorporated - ', '').str.replace('Los Angeles - ', '')\n",
    "df.sort_values('place_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# create a new column representing the proportion of cases that were fatal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. LA County Top Earners\n",
    "\n",
    "[Data source](https://data.lacounty.gov/Operations/Top-County-Earners/j7dj-b6in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = pd.read_csv('../../data/Top_County_Earners.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what do you see in the raw data?\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data types: do we need to change/convert any?\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# why does the total earnings column name above look weird?\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the total earnings column to something that won't trip you up\n",
    "df = df.rename(columns={' Total Earnings':'Total Earnings'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the float columns to ints: a couple ways you could do it (either works)...\n",
    "# OPTION 1: use IndexSlice from last week's lecture\n",
    "slicer = pd.IndexSlice[:, 'Base Earnings':'Total Compensation']\n",
    "df.loc[slicer] = df.loc[slicer].astype(int)\n",
    "\n",
    "# OPTION 2: select columns where type is float64\n",
    "float_cols = df.columns[df.dtypes=='float64']\n",
    "df[float_cols] = df[float_cols].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move year to end and employee name to beginning\n",
    "cols = [df.columns[-1]] + df.columns[1:-1].to_list() + [df.columns[0]]\n",
    "df = df.reindex(columns=cols)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert from USD to 1000s of USD\n",
    "df['Total Compensation 1000s'] = df['Total Compensation'] / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improve the capitalization (note, only Series can do vectorized str methods)\n",
    "slicer = pd.IndexSlice[:, 'Employee Name':'Department']\n",
    "df.loc[slicer] = df.loc[slicer].apply(lambda col: col.str.title(), axis='rows')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: you could use NLTK to classify male vs female names and examine average pay differences between the two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_earnings = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# convert all the earnings/compensation columns from USD to Euros, using today's exchange rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. LA City Active Businesses\n",
    "\n",
    "[Data source](https://data.lacity.org/Administration-Finance/Listing-of-Active-Businesses/6rrh-rzua)\n",
    "\n",
    "Note: [NAICS](https://en.wikipedia.org/wiki/North_American_Industry_Classification_System) is the North American Industry Classification System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = pd.read_csv('../../data/Listing_of_Active_Businesses.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what do you see in the raw data?\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data types: do we need to change/convert any?\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you have to make a decision: NAICS should be int, but it contains nulls\n",
    "# you could drop nulls then convert to int, or just leave it as float\n",
    "# OR in recent versions of pandas, you could cast to type pd.Int64Dtype() which allows nulls\n",
    "pd.isnull(df['NAICS']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure end dates are all null, then drop that column\n",
    "assert pd.isnull(df['LOCATION END DATE']).all()\n",
    "df = df.drop(columns=['LOCATION END DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the column names lower case and without spaces or hash signs\n",
    "cols = df.columns.str.lower().str.replace(' ', '_').str.strip('_#')\n",
    "df.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure account numbers are unique, then set as index and sort index\n",
    "assert df['location_account'].is_unique\n",
    "df = df.set_index('location_account').sort_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the start date from strings to datetimes\n",
    "df['location_start_date'] = pd.to_datetime(df['location_start_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improve the capitalization\n",
    "slicer = pd.IndexSlice[:, 'business_name':'mailing_city']\n",
    "df.loc[slicer] = df.loc[slicer].apply(lambda col: col.str.title(), axis='rows')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what's going on with those location coordinates?\n",
    "df['location'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the location column contains a mix of nulls and alongside strings of tuples of coordinates. Yikes. There are different ways to parse these coordinates out. Here's a relatively efficient option. First, some explanation:\n",
    "  \n",
    "  1. Create a mask of True/False identifying where `location` is not null, so we don't try to parse nulls.\n",
    "  2. Select all the non-null locations, `literal_eval` them (this \"runs\" each string as Python code, rendering them as tuples), and capture the result as a Series called `latlng`.\n",
    "  3. Create new `lat` and `lng` columns in `df` (only assigning values to them where the mask is True) by breaking-out the tuples from the previous step into a DataFrame with two columns.\n",
    "  4. Drop the now-redundant `location` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = pd.notnull(df['location'])\n",
    "latlng = df.loc[mask, 'location'].map(ast.literal_eval)\n",
    "df.loc[mask, ['lat', 'lng']] = pd.DataFrame(latlng.to_list(),\n",
    "                                            index=latlng.index,\n",
    "                                            columns=['lat', 'lng'])\n",
    "df = df.drop(columns=['location'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# create a new column containing only the 5-digit zip\n",
    "# which zip codes appear the most in the data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploration: description and visualization\n",
    "\n",
    "Python data visualization tool landscape:\n",
    "\n",
    "  - matplotlib is powerful but unwieldy; good for basic plotting (scatter, line, bar), and pandas can use it [directly](https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html)\n",
    "  - [seaborn](http://seaborn.pydata.org/) (built on top of matplotlib) is best for statistical visualization: summarizing data, understanding distributions, searching for patterns and trends\n",
    "  - [bokeh](https://docs.bokeh.org/) is for interactive visualization to let your audience explore the data themselves\n",
    "\n",
    "We will focus on **seaborn** in this class. It is the easiest to work with to produce meaningful and aesthetically-pleasing visuals. Seaborn makes generally smart decisions about color for you. But you can tweak the colors in your plot usually by passing in a `palette` argument (the name of a colormap or a list of colors to use). More info:\n",
    "\n",
    "  - How seaborn handles color: https://seaborn.pydata.org/tutorial/color_palettes.html\n",
    "  - Available color maps: https://matplotlib.org/tutorials/colors/colormaps.html\n",
    "  - Available named colors: https://matplotlib.org/gallery/color/named_colors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure seaborn's style for subsequent use\n",
    "sns.set_style('whitegrid') #visual styles\n",
    "sns.set_context('paper') #presets for scaling figure element sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our cleaned data sets from earlier\n",
    "print(df_business.shape)\n",
    "print(df_covid.shape)\n",
    "print(df_earnings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Understanding the data's distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick descriptive stats for some variable\n",
    "# but... looking across the whole population obscures between-group heterogeneity\n",
    "df_earnings['Total Compensation 1000s'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which departments have the most employees in the data set?\n",
    "dept_counts = df_earnings['Department'].value_counts().head()\n",
    "dept_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall grouping and summarizing from last week\n",
    "# look at compensation distribution across the 5 largest departments\n",
    "mask = df_earnings['Department'].isin(dept_counts.index)\n",
    "df_earnings.loc[mask].groupby('Department')['Total Compensation 1000s'].describe().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better... but it's still hard to pick out patterns and trends by just staring at a table full of numbers. Let's visualize it.\n",
    "\n",
    "**Box plots** illustrate the data's distribution via the \"5 number summary\": min, max, median, and the two quartiles (plus outliers). We will use seaborn for our visualization. In seaborn, you can control what's considered an outlier by changing min/max of whiskers with `whis` parameter... the convention is outliers > 1.5 IQR. For a vertical boxplot, x = your variable's column and y = categorical column to group by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize compensation distribution across the 5 largest departments\n",
    "x = df_earnings.loc[mask, 'Total Compensation 1000s']\n",
    "y = df_earnings.loc[mask, 'Department']\n",
    "\n",
    "# fliersize changes the size of the outlier dots\n",
    "# boxprops lets you set more configs with a dict, such as alpha (which means opacity)\n",
    "ax = sns.boxplot(x=x, y=y, fliersize=0.3, boxprops={'alpha':0.7})\n",
    "\n",
    "# set the x-axis limit, the figure title, and x/y axis labels\n",
    "ax.set_xlim(left=0)\n",
    "ax.set_title('Total compensation by department')\n",
    "ax.set_xlabel('Total compensation (USD, 1000s)')\n",
    "ax.set_ylabel('')\n",
    "\n",
    "# save figure to disk at 300 dpi and with a tight bounding box\n",
    "ax.get_figure().savefig('boxplot-earnings.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, your xlabel would state what year the USD are in (e.g., \"2017 inflation-adjusted USD\") but the data source doesn't say clearly. My guess is that they are nominal dollars from the reported year.\n",
    "\n",
    "What does this figure tell you? Which department had the highest total compensations? By what measure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is this \"ax\" variable we created?\n",
    "type(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# every matplotlib axes is associated with a \"figure\" which is like a container\n",
    "fig = ax.get_figure()\n",
    "type(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually change the plot's size/dimension by adjusting its figure's size\n",
    "fig = ax.get_figure()\n",
    "fig.set_size_inches(16, 4) #width, height in inches\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Histograms** visualize the distribution of some variable by binning it then counting observations per bin. KDE plots are similar, but continuous and smooth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histplot visualizes the variable's distribution as a histogram and optionally a KDE\n",
    "ax = sns.histplot(df_earnings['Total Compensation 1000s'].dropna(), kde=False, bins=30)\n",
    "_ = ax.set_xlim(left=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can compare multiple histograms to see how different groups overlap or differ by some measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# typical LASD employee earns more than the typical regional planner :(\n",
    "df_earnings.groupby('Department')['Total Compensation 1000s'].median().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visually compare sheriff and social services dept subsets\n",
    "mask = df_earnings['Department'].isin(['Public Social Services Dept', 'Sheriff'])\n",
    "ax = sns.histplot(data=df_earnings.loc[mask],\n",
    "                  x='Total Compensation 1000s',\n",
    "                  hue='Department',\n",
    "                  bins=50,\n",
    "                  kde=False)\n",
    "\n",
    "ax.set_xlim(0, 400)\n",
    "ax.set_xlabel('Total compensation (USD, 1000s)')\n",
    "ax.set_title('Employee Compensation: LASD vs Social Services')\n",
    "ax.get_figure().savefig('boxplot-hists.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like a pretty big difference! But is it statistically significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference-in-means: compute difference, t-statistic, and p-value\n",
    "group1 = df_earnings[df_earnings['Department']=='Public Social Services Dept']['Total Compensation 1000s']\n",
    "group2 = df_earnings[df_earnings['Department']=='Sheriff']['Total Compensation 1000s']\n",
    "t, p = stats.ttest_ind(group1, group2, equal_var=False, nan_policy='omit')\n",
    "print(group1.mean() - group2.mean(), t, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Social service workers in LA county make, on average, $56k less than LASD employees and this difference is statistically significant (p<0.001).\n",
    "\n",
    "Note also that you can divide your p-value by 2 if you need to convert it from a two-tailed to one-tailed hypothesis test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the big reveal... who (individually) had the highest earnings?\n",
    "cols = ['Employee Name', 'Position Title', 'Department', 'Total Compensation 1000s']\n",
    "df_earnings[cols].sort_values('Total Compensation 1000s', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# choose 3 departments and visualize their overtime earnings distributions with histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Pairwise relationships\n",
    "\n",
    "Histograms and box plots visualize univariate distributions: how a single variable's values are distributed. Scatter plots essentially visualize *bivariate* distributions so that we can see patterns and trends jointly between two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use seaborn to scatter-plot two variables\n",
    "ax = sns.scatterplot(x=df_covid['cases_final'],\n",
    "                     y=df_covid['deaths_final'])\n",
    "ax.set_xlim(left=0)\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.get_figure().set_size_inches(5, 5) #make it square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a pair plot of these SF tracts across these 4 variables\n",
    "cols = ['cases_final', 'deaths_final', 'population']\n",
    "ax = sns.pairplot(df_covid[cols].dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do you see patterns in these scatter plots?** *Correlation* tells us to what extent two variables are linearly related to one another. Pearson correlation coefficients range from -1 to 1, with 0 indicating no linear relationship, -1 indicating a perfect negative linear relationship, and 1 indicating a perfect positive linear relationship. If you are hypothesis-testing a correlation, make sure to report and interpret the p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate correlation (and significance) between two variables\n",
    "r, p = stats.pearsonr(x=df_covid['population'], y=df_covid['cases_final'])\n",
    "print(round(r, 3), round(p, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a correlation matrix\n",
    "correlations = df_covid[cols].corr()\n",
    "correlations.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visual correlation matrix via seaborn heatmap\n",
    "# use vmin, vmax, center to set colorbar scale properly\n",
    "ax = sns.heatmap(correlations, vmin=-1, vmax=1, center=0,\n",
    "                 cmap='coolwarm', square=True, linewidths=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# visualize a correlation matrix of the various compensation columns in the earnings dataframe\n",
    "# from the visualize, pick two variables, calculate their correlation coefficient and p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regress one variable on another: a change in x is associated with what change in y?\n",
    "m, b, r, p, se = stats.linregress(x=df_covid['population'], y=df_covid['cases_final'])\n",
    "print(m, b, r, p, se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a linear (regression) trend line + confidence interval\n",
    "ax = sns.regplot(x=df_covid['population'], y=df_covid['cases_final'])\n",
    "ax.get_figure().set_size_inches(5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# does logarithmic transformation improve the heteroskedasticity and linear fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Bar plots and count plots\n",
    "\n",
    "Count plots let you count things across categories.\n",
    "\n",
    "Bar plots let you estimate a measure of central tendency across categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the two-digit sector code from each NAICS classification\n",
    "sectors = df_business['naics'].dropna().astype(int).astype(str).str.slice(0, 2)\n",
    "sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count plot: like a histogram counting observations across categorical instead of continuous data\n",
    "order = sectors.value_counts().index\n",
    "ax = sns.countplot(x=sectors, order=order, alpha=0.9, palette='plasma')\n",
    "ax.set_xlabel('NAICS Sector')\n",
    "ax.set_ylabel('Number of businesses')\n",
    "ax.get_figure().savefig('countplot-naics.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAICS sector 54 is \"professional, scientific, and technical services\" and sector 53 is \"real estate and rental and leasing.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot: estimate mean total compensation per dept + 95% confidence interval\n",
    "order = df_earnings.groupby('Department')['Total Compensation 1000s'].mean().sort_values(ascending=False).index\n",
    "ax = sns.barplot(x=df_earnings['Total Compensation 1000s'],\n",
    "                 y=df_earnings['Department'],\n",
    "                 estimator=np.mean,\n",
    "                 errorbar=('ci', 95),\n",
    "                 order=order,\n",
    "                 alpha=0.9)\n",
    "\n",
    "ax.set_xlabel('Mean Total Compensation (USD, 1000s)')\n",
    "ax.set_ylabel('')\n",
    "ax.get_figure().set_size_inches(4, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# use the businesses dataframe to visualize a bar plot of mean start year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Line plots\n",
    "\n",
    "Line plots are most commonly used to visualize time series: how one or more variables change over time. We don't have time series data here, so we'll improvise with a bit of an artificial example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract years from each start date then count their appearances\n",
    "years = df_business['location_start_date'].dropna().dt.year.value_counts().sort_index()\n",
    "years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reindex so we're not missing any years\n",
    "labels = range(years.index.min(), years.index.max() + 1)\n",
    "years = years.reindex(labels).fillna(0).astype(int)\n",
    "years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line plot showing counts per start year over past 40 years\n",
    "ax = sns.lineplot(data=years.loc[1980:2020])\n",
    "\n",
    "# rotate the tick labels\n",
    "ax.tick_params(axis='x', labelrotation=45)\n",
    "\n",
    "ax.set_xlim(1980, 2020)\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Business Location Starts by Year')\n",
    "ax.get_figure().savefig('lineplot-businesses.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# extract month + year from the original date column\n",
    "# re-create the line plot to visualize location starts by month + year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ppd599)",
   "language": "python",
   "name": "ppd599"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
